# Fault-Tolerant Data Processing System

A robust data ingestion and processing service that handles unreliable data from multiple clients with idempotency guarantees, normalization, and fault tolerance.

## ğŸš€ Quick Start

### Installation

```bash
npm install
```

### Running the Application

```bash
npm start
```

Visit `http://localhost:3000` in your browser.

## ğŸ“‹ What Assumptions Did I Make?

### 1. Data Model Assumptions
- **Required Fields**: Every event must contain client identifier, metric type, amount, and timestamp (in any of their aliased forms)
- **Semantic Uniqueness**: Two events with identical semantic content (client, metric, amount, timestamp) are considered duplicates, regardless of when they were received
- **Amount Interpretation**: All amounts represent positive numeric values that can be summed for aggregation
- **Timestamp Flexibility**: Timestamps without timezone information are assumed to be UTC

### 2. System Behavior Assumptions
- **Retry Strategy**: Clients will retry failed requests with identical payloads
- **No Out-of-Order Guarantees**: Events may arrive in any order; the system doesn't enforce temporal ordering
- **Best-Effort Normalization**: Unknown or extra fields are logged as warnings but don't cause failures
- **Single Instance**: The system runs as a single process (no distributed coordination needed)

### 3. Scale Assumptions
- **Event Volume**: Designed for thousands of events per hour, not millions per second
- **Client Count**: Optimized for 10-100 clients, not 10,000
- **Data Retention**: All data is kept indefinitely (no automatic archival/deletion)
- **Query Patterns**: Aggregations are computed on-demand (no pre-aggregation)

### 4. Operational Assumptions
- **Database**: SQLite is sufficient for this scale (single-file, embedded database)
- **Deployment**: Single-server deployment (no load balancing required)
- **Monitoring**: Application logs are sufficient for debugging (no distributed tracing)

## ğŸ”’ How Does the System Prevent Double Counting?

The system uses a **content-based hashing strategy** combined with database constraints to ensure idempotency:

### 1. Content Hash Generation
```javascript
// Generate deterministic hash from event content
const eventHash = SHA256(normalize(rawEvent))
```

- **Deterministic**: Same event content always produces the same hash
- **Semantic**: Based on business data (client, metric, amount, timestamp), not metadata
- **Collision-Resistant**: SHA-256 provides cryptographic guarantees

### 2. Database-Level Uniqueness
```sql
CREATE TABLE raw_events (
    event_hash TEXT UNIQUE NOT NULL,  -- Prevents duplicates at DB level
    ...
);
```

- **UNIQUE Constraint**: Database rejects duplicate hashes
- **Transaction Safety**: Hash check and insert happen atomically

### 3. Three-Step Processing Pipeline

```
1. CHECK: Query for existing event_hash
   â”œâ”€ Found â†’ Return existing result (idempotent)
   â””â”€ Not found â†’ Continue to step 2

2. NORMALIZE: Validate and transform data
   â”œâ”€ Invalid â†’ Store as failed event
   â””â”€ Valid â†’ Continue to step 3

3. PERSIST: Store in transaction
   â”œâ”€ Transaction BEGIN
   â”œâ”€ Insert raw_event (with UNIQUE constraint)
   â”œâ”€ Insert normalized_event
   â”œâ”€ Transaction COMMIT
   â””â”€ On failure â†’ ROLLBACK (nothing persisted)
```

### 4. Retry Behavior Example

**First Attempt:**
```
Client sends: { "source": "A", "amount": "100" }
Hash: abc123...
DB: Insert successful
Response: 201 Created
```

**Retry (Duplicate):**
```
Client sends: { "source": "A", "amount": "100" }
Hash: abc123... (same!)
DB: Hash already exists
Response: 200 OK (idempotent, no new processing)
```

**Modified Event:**
```
Client sends: { "source": "A", "amount": "200" }
Hash: xyz789... (different!)
DB: Insert successful
Response: 201 Created (new event)
```

### 5. Why This Prevents Double Counting

- âœ… **Same event sent twice**: Hash matches â†’ deduped before insertion
- âœ… **Partial failure + retry**: Hash matches â†’ returns existing result
- âœ… **Network timeout + retry**: Hash matches â†’ idempotent response
- âœ… **Client bug sending duplicates**: Hash matches â†’ counted once
- âŒ **Different events with same hash**: Statistically impossible (SHA-256)

## ğŸ”§ What Happens If the Database Fails Mid-Request?

The system uses **database transactions** to ensure atomic operations. Here's what happens in different failure scenarios:

### Scenario 1: Failure Before Transaction
```
Request arrives
â†“
Generate hash: abc123
â†“
Check for duplicate â†’ DB query fails âŒ
â†“
Response: 500 Internal Server Error
Effect: No data written, no state change
```
**Outcome**: Client retries, event processes successfully on retry.

### Scenario 2: Failure During Transaction
```
Transaction BEGIN
â†“
Insert raw_event â†’ Success âœ“
â†“
Insert normalized_event â†’ Fails âŒ
â†“
Transaction ROLLBACK (automatic)
â†“
Response: 500 Internal Server Error
Effect: No data written (both inserts rolled back)
```
**Outcome**: Client retries, event processes successfully on retry.

### Scenario 3: Failure After Transaction Commit
```
Transaction BEGIN
â†“
Insert raw_event â†’ Success âœ“
â†“
Insert normalized_event â†’ Success âœ“
â†“
Transaction COMMIT â†’ Success âœ“
â†“
Network failure before response âŒ
```
**Outcome**: Event IS persisted, but client doesn't know. On retry, hash matches â†’ returns existing result (idempotent).

### Key Safety Properties

1. **Atomicity**: Both raw and normalized events are saved together or not at all
   - Achieved through: `db.transaction()` wrapper
   - Guarantee: No partial writes

2. **Consistency**: Event counts are always accurate
   - Duplicates are detected before insertion
   - Failed events are tracked separately
   - Aggregations only include successfully processed events

3. **Idempotency**: Retries are safe and don't cause duplication
   - Hash is checked before any writes
   - Same request â†’ same response
   - Client can retry freely

4. **Failure Tracking**: Failed events are logged for investigation
   ```sql
   INSERT INTO failed_events (
       event_hash, raw_data, error_message, error_type
   )
   ```
   - Captures: What failed, why it failed, when it failed
   - Allows: Manual reprocessing or investigation

### Testing Failure Handling

The UI includes a "Simulate Database Failure" option:

```javascript
// In eventService.js
if (simulateFailure) {
    throw new Error('Simulated database failure');
}
```

**Try it:**
1. Submit event with "Simulate Failure" checked
2. Observe 500 error response
3. Uncheck "Simulate Failure" and submit same event
4. Observe successful processing (no duplicate)

## ğŸ“Š What Would Break First at Scale?

### 1. SQLite Database (Most Likely First Failure)
**Breaks at**: ~10,000 concurrent writes/second

**Why**:
- SQLite uses file-level locking
- Write transactions serialize (one at a time)
- No horizontal scaling

**Symptoms**:
```
Error: database is locked
Response times: 1s â†’ 5s â†’ timeouts
```

**Solutions**:
- **Short-term**: Increase timeout, add write-ahead logging (already enabled)
- **Long-term**: Migrate to PostgreSQL/MySQL (allows concurrent writes)

### 2. In-Memory Aggregations (Second Failure)
**Breaks at**: ~1 million events

**Why**:
- Aggregations compute on every query
- No caching or pre-aggregation
- Full table scans on large datasets

**Symptoms**:
```
Query times: 100ms â†’ 5s â†’ 30s
CPU usage: High
Memory: OK (queries stream results)
```

**Solutions**:
- **Short-term**: Add query result caching (Redis)
- **Medium-term**: Materialized views or pre-aggregated tables
- **Long-term**: Time-series database (TimescaleDB, InfluxDB)

### 3. Single-Server Architecture (Third Failure)
**Breaks at**: Server resource limits (CPU, memory, network)

**Why**:
- No redundancy
- No load balancing
- Single point of failure

**Symptoms**:
```
High CPU: Event processing slows
High memory: Risk of OOM crashes
Server down: Complete outage
```

**Solutions**:
- **Short-term**: Vertical scaling (bigger server)
- **Long-term**: Horizontal scaling (multiple servers + message queue)

### 4. Hash Collision (Theoretical, Never in Practice)
**Breaks at**: Never (2^128 events needed for 50% collision probability)

**Why**:
- SHA-256 has 256-bit output space
- Astronomically unlikely

**If it happened**:
- Different events would be treated as duplicates
- Symptom: Events mysteriously "already processed"

### Scaling Roadmap

| Scale | Events/Day | Architecture | Database |
|-------|-----------|--------------|----------|
| **Current** | <100K | Single process | SQLite |
| **Phase 1** | <1M | Single server | PostgreSQL |
| **Phase 2** | <10M | Multi-server + Queue | PostgreSQL + Redis |
| **Phase 3** | >10M | Microservices | PostgreSQL + TimescaleDB |

### What to Monitor

1. **Database Performance**
   ```
   - Write latency (p50, p95, p99)
   - Lock wait time
   - Connection pool usage
   ```

2. **API Performance**
   ```
   - Request latency
   - Throughput (requests/second)
   - Error rate
   ```

3. **Resource Usage**
   ```
   - CPU utilization
   - Memory usage
   - Disk I/O
   ```

4. **Business Metrics**
   ```
   - Duplicate rate (should be stable)
   - Validation failure rate
   - Processing success rate
   ```

## ğŸ—ï¸ Architecture Overview

### Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Frontend (UI)                  â”‚
â”‚  â”œâ”€ Event Submission Form                       â”‚
â”‚  â”œâ”€ Statistics Dashboard                        â”‚
â”‚  â””â”€ Aggregation Views                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ HTTP/JSON
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Express API Server                  â”‚
â”‚  â”œâ”€ POST /api/events (ingestion)                â”‚
â”‚  â”œâ”€ GET  /api/events (query)                    â”‚
â”‚  â”œâ”€ GET  /api/aggregations                      â”‚
â”‚  â””â”€ GET  /api/stats                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EventService â”‚    â”‚ AggregationServiceâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€ Normalizer (validation, type coercion)
       â”œâ”€ IdempotencyHandler (hashing, deduplication)
       â””â”€ Database (SQLite)
          â”œâ”€ raw_events (original data)
          â”œâ”€ normalized_events (processed data)
          â”œâ”€ failed_events (validation failures)
          â””â”€ processing_log (audit trail)
```

### Data Flow

```
1. Raw Event Ingestion
   â”œâ”€ Generate content hash
   â”œâ”€ Check for duplicate (hash lookup)
   â”œâ”€ Normalize data (validation + transformation)
   â””â”€ Persist in transaction

2. Query & Aggregation
   â”œâ”€ Filter by client/date/status
   â”œâ”€ Group and aggregate
   â””â”€ Return JSON

3. Failure Handling
   â”œâ”€ Validation failure â†’ failed_events table
   â”œâ”€ Persistence failure â†’ transaction rollback
   â””â”€ Retry â†’ idempotent response
```

## ğŸ§ª Testing

### Manual Testing with UI

1. **Basic Event**:
   ```json
   {
     "source": "client_A",
     "payload": {
       "metric": "revenue",
       "amount": "1200",
       "timestamp": "2024/01/01"
     }
   }
   ```

2. **Duplicate Detection**:
   - Submit same event twice
   - Second submission should show "Duplicate Detected"

3. **Failure Simulation**:
   - Check "Simulate Database Failure"
   - Submit event â†’ 500 error
   - Uncheck and submit again â†’ Success (no duplicate)

4. **Invalid Data**:
   ```json
   {
     "source": "client_A",
     "payload": {
       "metric": "revenue",
       "amount": "invalid_number"
     }
   }
   ```
   - Should show validation errors

### API Testing with cURL

```bash
# Submit event
curl -X POST http://localhost:3000/api/events \
  -H "Content-Type: application/json" \
  -d '{"source":"client_A","payload":{"metric":"test","amount":"100","timestamp":"2024/01/01"}}'

# Get events
curl http://localhost:3000/api/events

# Get aggregations
curl http://localhost:3000/api/aggregations

# Get statistics
curl http://localhost:3000/api/stats
```

## ğŸ¯ Design Decisions

### Why SQLite?
- âœ… Zero configuration
- âœ… Single file, easy to backup
- âœ… ACID transactions built-in
- âœ… Sufficient for assignment scale
- âŒ Limited concurrency (acceptable trade-off)

### Why Content-Based Hashing?
- âœ… Deterministic deduplication
- âœ… Works without unique event IDs
- âœ… Survives partial failures
- âŒ Can't distinguish intentional re-submissions (acceptable)

### Why Separate Raw and Normalized Tables?
- âœ… Audit trail of original data
- âœ… Can reprocess if normalization logic changes
- âœ… Failed events can be investigated
- âŒ Slight storage overhead (acceptable)

### Why Transaction-Based Persistence?
- âœ… Atomic guarantees
- âœ… Simple failure handling
- âœ… No complex state machines
- âŒ Lower throughput (acceptable for scale)

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ server.js                 # Express app and route definitions
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ database.js           # SQLite setup and schema
â”‚   â”œâ”€â”€ normalizer.js         # Data normalization logic
â”‚   â”œâ”€â”€ idempotencyHandler.js # Hashing and deduplication
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ eventService.js   # Event ingestion orchestration
â”‚       â””â”€â”€ aggregationService.js # Query and aggregation logic
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html            # Frontend UI
â”‚   â”œâ”€â”€ styles.css            # Styling
â”‚   â””â”€â”€ app.js                # Frontend JavaScript
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ”‘ Key Features

- âœ… Idempotent event ingestion
- âœ… Content-based deduplication
- âœ… Flexible data normalization
- âœ… Transaction-based consistency
- âœ… Partial failure handling
- âœ… Real-time aggregations
- âœ… Failure simulation for testing
- âœ… Comprehensive audit trail

## ğŸš§ Future Enhancements

### Short-term
1. Add retry queue for failed events
2. Implement query result caching
3. Add more aggregation types (percentiles, time-series)
4. Export functionality (CSV, JSON)

### Medium-term
1. Migrate to PostgreSQL for better concurrency
2. Add authentication and authorization
3. Rate limiting per client
4. Webhook notifications for failures

### Long-term
1. Distributed deployment with message queue
2. Streaming aggregations
3. ML-based anomaly detection
4. Time-series optimized storage

---

**Built with**: Node.js, Express, SQLite, Vanilla JavaScript

**Design Principles**: ACID transactions, idempotency, separation of concerns, fail-safe defaults
